---
title: "Brazil Forest Fires"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:/Users/dex78/Documents/Dexter/Projects/Brazil-Forest-Fires/forest_fire_app")
options(warn=-1)
```

## Intro

This notebook is for annotating my process for creating my Shiny web app. The app will be capable of generating graphs and maps from the Amazon/Brazil forest fire data.

## Choropleth Map

I was initially going to download geojson for Brazil's states, but thought that there was probably a simpler soloution. Soon after, I found that the makers of ggplot also created ggmap. ggmap allows fairly easy mapping without the need to download geographic data directly, instead using Google APIs. While this would enable me to create nice maps with zoom and drag capabilities, I would like people using the app to easily start it up without creating their own API keys. I'm not about to publish my API key on Github because Google could charge me if someone were to abuse it. Alternatively, I could host the project online and hide the key on Github, but I won't for now. ggmap is not looking ideal for this project, so I will continue to look for other options. 

After a bit of searching, I've discovered the CRAN package "brazilmaps", however this is not a widely used package and hasn't been updated in the past two years. Although its online documentation isn't great, there is some which I appreciate -- it makes using their functions much easier. 

Now that I've gotten a handle on the format of brazil maps and can map by state:
```{r, message=F}
library(brazilmaps)
library(dplyr)
library(ggplot2)
# must be loaded for get_brmap to work
library(sf)
map_states <- get_brmap(geo = "State", class = "sf")
plot_brmap(map_states)
```

I need to determine how to overlap this data with the Kaggle data. Looking at map_states' names, I found "nome" ("name" in Portuguese) and "state" from the Kaggle data represent the names of Brazil's states. I also noticed that the Kaggle data was not in all caps and the vector type was not in factors, which is easy enough to fix.

Great, now to test how our current data maps onto the states. This can be joined done with the plot_brmap function, given some extra parameters:

```{r, warning=F}
BR_fire_data <- read.csv('../data/original.csv')
BR_fire_data$state <- as.factor(toupper(BR_fire_data$state))
plot_brmap(map_states,
           data_to_join = BR_fire_data,
           join_by = c('nome'='state'),
           var = 'number')
```

### Preprocessing

While this does show a cloropleth map, it doesn't show as many states as we might have expected. Maybe there's a discresion in the names - let's check.

```{r}
# Print the state names from each vector of factors
cat(paste('Levels from Kaggle data:',
          paste0(levels(BR_fire_data$state), collapse=", "),
          '',
          'Levels from brazilmaps:',
          paste0(levels(map_states$nome), collapse=", "),
          sep = '\n'))
```

After looking at the levels, a clear difference is the accents for the names generated by the brazilmaps package. Is it the only difference? Even if we hadn't noticed that the brazilmaps levels list is longer, we can spot the differences in the two lists with the following code:
"RIO DE JANEIRO", "RIO GRANDE DO NORTE" and "RIO GRANDE DO SUL".

```{r}
# remove accents
levels(map_states$nome) <- iconv(levels(map_states$nome),from="UTF-8",to="ASCII//TRANSLIT")
levels(BR_fire_data$state) <- iconv(levels(BR_fire_data$state),from="UTF-8",to="ASCII//TRANSLIT")

# return vector of state names in brazilmaps that are not in Kaggle
cat("In brazilmaps but not Kaggle:\n")
cat(
  paste0(
    setdiff(
      levels(map_states$nome),
      levels(BR_fire_data$state)),
    collapse=", "
    )
  )

# Equivalent, except from Kaggle to brazilmaps
cat("\n\nIn Kaggle but not brazilmaps:\n")
cat(
  paste0(
    setdiff(
      levels(BR_fire_data$state), 
      levels(map_states$nome)),
    collapse=", "
    )
  )
```

A quick check of Wikipedia tells us that that Piaui is correct, not Piau. It also appears that there is 'duplicate' data for RIO and MATO GROSSO which indicates that data for all 3 Rios and 2 Mato Grossos exist, despite their labelling. The only remaining states that aren't understood are PARA and PARANA.

Let's reload and examine how many each state occurs in the Kaggle data.

```{r}
BR_fire_data <- read.csv('../data/original.csv')
BR_fire_data %>% 
  group_by(state) %>%
  summarise(no_rows = length(state))
```

The reason for reloading was because 'Par√°' actually produced an error from our earlier transformations. PARANA must have been labelled as Paraiba, since it's the only unexplained double-count factor. 

So how should we handle this imperfect mapping? If it was complex, it might be easier to leave the data as is or to join the duplicate data. We won't do this because the problem isn't too challenging and some of the mislabelled geographies such as the three Rios are NOT close. Let's cross-reference the data with another reliable source. Note that the data does not line up exactly.

Using a visualization from [globalforestwatch](http://fires.globalforestwatch.org/report/index.html#aoitype=GLOBAL&reporttype=globalcountryreport&country=Brazil&dates=fYear-2019!fMonth-6!fDay-1!tYear-2019!tMonth-11!tDay-27), here are the total number of fires for 2003 for our states of interest:

State | GFW Fires in 2003
------------- | -------------
Rio de Janeiro | 1,786
Rio Grande do Norte | 2,139
Rio Grande do Sul | 7,037
Parana | 12,756
Paraiba | 2,190
Mato Grosso | 10,737
Mato Grosso do Sul | 150,256

Now I'll add the suspected Kaggle data to each column. Rio de Janeiro and Rio Grande do Norte have somewhat similar numbers, so the comparison will be done with 2003 and 2017 data.

State | GFW Fires,2003 | Kaggle Fires,2003 | GFW Fires,2007 | Kaggle Fires,2007
------------- | ------------- | ------------- | ------------- | -------------
Rio de Janeiro | 1,786 | 553 | 2,400 | 621 
Rio Grande do Norte | 2,139 | 1,262 | 1,230 | 463
Rio Grande do Sul | 7,037 | 3,041 | 3,488 | 983
Parana | 12,756 | 6,980 | 7,028 | 2,057
Paraiba | 2,190 | 1,381 | 1,430 | 608
Mato Grosso | 10,737 | 4,130 | 21,782 | 7,747
Mato Grosso do Sul | 150,256 | 35,406 | 164,789 | 44,406

Although there is some difference in how much the states differ, these two comparisons make it look like my inference is correct. If GRW provided me with their data, the comparison could be done with more points or with visualization to make my assumptions more convincing. 

The date column isn't accurate - all dates are January 1 of the appropriate year. For now I will delete this column, but I will recreate using the year and month column if it's worth having in date format.

After looking at the number of fires columns, you might also notice that a period is used in certain instances, often in groups and close to numbers close to 1000. This indicates they are being used in place of a comma or nothing since it's supposed to be an integer. So transformations like $1.234 \rightarrow 1234$ must be made. 

Initially, you might think replacing '.' with '' will deal with our problem, however it does not -- there are some points with only one or two decimals. They must have ended in zero. It's hard to find those with the off chance of being a multiple of 1000, but we'll cross that bridge when we get there. The change $1.23 \rightarrow 1230$ may seem daunting if:

1. You were to change all 800+ manually.
2. You are not experienced with these weird preprocessing problems.

Luckily, there's something called $\textit{Regular Expressions}$ (RegEx) that can be used to solve this. This [Medium article by Jonny Fox](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285) does a great job at introducing the topic, but I will assume the reader knows basic regex. The following are the steps used to transform the numbers in the data set:

1. Open the original csv file (data folder) in a text editor with RegEx Find & Replace.
2. Search for the desired numbers using RegEx 
   "#.#," found by "(\d)\.(\d)" where the round brackets are used for grouping
   Similarly "#.##," and "#.###," can be found using "(\\d)\.(\\d{2})" and "(\\d)\\.(\\d{2})", respectively.
3. Use your groups combined with any supplementary "0"s to properly format the numbers.
ie. "$1$200", "$1$20" and "$1$2" for the above situations.

Now how might we find a number that should actually be #.000 only the ".000" isn't there? Well, we can do a check using regex but we won't know for sure. Before doing that, sort the data by Month,Year,State so that we see the data in sequence, by state. This is important for spotting odd numbers in context. Here's the Regex search I used: 

\begin{align}
,\setminus d\{3,4\}\setminus n\setminus d*,(\setminus w*,)\{2\}\setminus d\setminus n\setminus d*,(\setminus w*,)\{2\}\setminus d\{3,4\}
\end{align}

where I'm searching for a single digit number preceeded and followed by a 3-4 digit number. The same principle can be used to search for a number like 55000.

There are 4 matches, but the only one I'd be confident changing would be the middle row below:

Year | State | Month | Number of Fires
---- | ----- | ----- | ---------------
2012 | MARANHAO | September | 7626
2012 | MARANHAO | October | 5     (should probably be 5000)
2012 | MARANHAO | November | 2010

The following also looks suspicious, but I'll attribute that to a measurement error - jumping to 5000 seems unlikely given surrounding numbers.

Year | State | Month | Number of Fires
---- | ----- | ----- | ---------------
2009 | MARANHAO | October | 1861
2009 | MARANHAO | November | 5
2009 | MARANHAO | December | 1768

I dug a bit deeper into other possible multiples of 1000 but did not see anything I previously missed.

After the transformations we layed out are complete (renaming, capitalizing, adding accents and reformatting numbers), the original data becomes amazon.csv. 

Here is the graph we tried to make at the start of this file:

```{r}
# must specify the incoding, or the accents won't be read properly
BR_fire_data <- read.csv('../data/amazon.csv', encoding="UTF-8")
BR_fire_data$state <- as.factor(BR_fire_data$state)
map_states <- get_brmap(geo = "State", class = "sf")
plot_brmap(map_states,
           data_to_join = BR_fire_data,
           join_by = c('nome'='state'),
           var = 'Fires',) + 
  scale_fill_gradientn(limits = c(0,10000),
                       colours=c("#FFFF00", "#FF0000")
                       )
```

### Considerations

Now that the hard work has been done (besides the data, I've also been having problems with brazilmaps), we can begin producing more standard graphs. Our graph above doesn't look quite right, but that can be examined at a later time. I'm not completely sure of how the data was aggregated, but the graph looks more or less as desired. If you ran this yourself, you'd probably realize it's incredibly slow. An improvement I plan to make is taking the data and ploting it with my own function.In preparation of that, I've added the Brazilian state data taken from the [brazilmaps](https://github.com/rpradosiqueira/brazilmaps) repo in the data folder.

```{r}
# basic plot of Brazilian states
plot(st_geometry(readRDS('data/brazil_states.Rds')))
```

There are multiple reasons the brazilmaps plotting could be slow. It could simply be that aggregating the data takes a long time, ggplot is slow with the geom_polygons or maybe it's the fault of the function's implementation. I've made several graphs in R with larger data and they didn't cause such long wait times, however I have not attempted choropleths with ggplot before. I can test this out with a simple table of data with the states, but I'm going to move on to plotting other graphs.

After attempting to create line graphs with the months as strings, I think it's worthwhile to recreate the dates column properly. First, transform the month-names to their appropriate month-numbers, then run the following code:

```{r}
BR_df <- read.csv('data/amazon.csv', encoding="UTF-8")
BR_df$date <- as.Date(with(BR_df, paste(year, month, '01',sep="-")), "%Y-%m-%d")
# save (will be renamed as amazon.csv once we know it's as desired)
# write.csv(BR_df, 'data/amazon_with_date.csv')
BR_df
```

If this is satisfactory and the graphs can be done with only the date column, they can be deleted. For now, I will keep them until I am sure I don't need them. I'll also reorganize this notebook so that the process is clearer.

## Menu for Graph Creation

The menu has been created based off of general graph types, each with the specific graphs that will be available to users. I also added filters based on Brazilian state, year and month for now. I plan on adding a date range, because maybe a specific range is more meaningful than year totals or month totals. 

I started off with the line graph, because it deals with similar aggregation and filtering that I will have to deal with in other graph types. Filtering by state was relatively easy, but properly aggregating by year or by date required me to better understand the Date type in R. I ended up using the lubridate library because it provides more versitile functions than the simple base R ones. 

I had a misconception that like SQL, the dplyr group_by and summerize functions would solve my aggregation problem without incident. What I didn't understand was that the summerize function requires one value for each column in the result. I also found it difficult to use the aggregate function in the same pipe as the filters, so I ended up doing that as a seperate step. 

```{r}
# sums fire by year and by state
aggregate(fires ~ year + state, data = df, sum)
```

The syntax might not be intuitive, but the column (or columns if you use the cbind(col1, col2,...)) before the '~' (tilde) must be numeric (or else their count will be shown instead) and are summed, by the distinct values mentioned (seperated by '+') after the tilde. Note that if you do not specify data, your columns should not be typed independent of their data.frame (write df$fires, instead of fires).

### Line & Jitter Graphs
For this app, I'm giving the Line and Jitter graph the same options, namely choosing a date range, summing (grouping for jitter) by month or year and selecting which states and months/years to include. After a bit of time deliberating on the menu options, I have to say it turned out quite well.

I should say, I had a simple-but-difficult-to-find error with the x-axis because the date column was not actually being read as a date by default. I handle this on app start with the line:
```{r}
BR_df$date <- as.Date(BR_df$date)
```

Note: If you select a date range with the Jitter plot, you will end up with a normal scatter plot since data points are not grouped. 

### Bar Graph Over Time
Not a challenge, since the data is grouped in a similar manner to the Jitter graph. Similar to basically all ggplots, geom_bar was what needed to be added to the base ggplot object I created a couple steps before displaying. For aesthetics ("aes"), fill is what the state column was assigned to in order to seperate their data. The other bar graph I plan on creating could probably fit in the same condition statement that the previous three graphs went in, but I plan on seperating it with more similar graphs (focus on which state the data comes from) for readability purposes.






