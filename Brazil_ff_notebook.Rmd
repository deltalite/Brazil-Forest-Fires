---
title: "R Notebook"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:/Users/dex78/Documents/Dexter/Projects/Brazil-Forest-Fires/forest_fire_app")
options(warn=-1)
```

This notebook is for annotating my process for creating my Shiny web app. The app will be capable of generating graphs and maps from the Amazon/Brazil forest fire data.

I was initially going to download geojson for Brazil's states, but thought that there was probably a simpler soloution. Soon after, I found that the makers of ggplot also created ggmap. ggmap allows fairly easy mapping without the need to download geographic data directly, instead using Google APIs. While this would enable me to create nice maps with zoom and drag capabilities, I would like people using the app to easily start it up without creating their own API keys. I'm not about to publish my API key on Github because Google could charge me if someone were to abuse it. Alternatively, I could host the project online and hide the key on Github, but I won't for now. ggmap is not looking ideal for this project, so I will continue to look for other options. 

After a bit of searching, I've discovered the CRAN package "brazilmaps", however this is not a widely used package and hasn't been updated in the past two years. Although its online documentation isn't great, there is some which I appreciate -- it makes using their functions much easier. 

Now that I've gotten a handle on the format of brazil maps and can map by state:
```{r, message=F}
library(brazilmaps)
library(dplyr)
# must be loaded for get_brmap to work
library(sf)
map_states <- get_brmap(geo = "State", class = "sf")
plot_brmap(map_states)
```

I need to determine how to overlap this data with the Kaggle data. Looking at map_states' names, I found "nome" ("name" in Portuguese) and "state" from the Kaggle data represent the names of Brazil's states. I also noticed that the Kaggle data was not in all caps and the vector type was not in factors, which is easy enough to fix.

Great, now to test how our current data maps onto the states. This can be joined done with the plot_brmap function, given some extra parameters:

```{r, warning=F}
BR_fire_data <- read.csv('../data/original.csv')
BR_fire_data$state <- as.factor(toupper(BR_fire_data$state))
plot_brmap(map_states,
           data_to_join = BR_fire_data,
           join_by = c('nome'='state'),
           var = 'number')
```

While this does show a cloropleth map, it doesn't show as many states as we might have expected. Maybe there's a discresion in the names - let's check.

```{r}
# Print the state names from each vector of factors
cat(paste('Levels from Kaggle data:',
          paste0(levels(BR_fire_data$state), collapse=", "),
          '',
          'Levels from brazilmaps:',
          paste0(levels(map_states$nome), collapse=", "),
          sep = '\n'))
```

After looking at the levels, a clear difference is the accents for the names generated by the brazilmaps package. Is it the only difference? Even if we hadn't noticed that the brazilmaps levels list is longer, we can spot the differences in the two lists with the following code:
"RIO DE JANEIRO", "RIO GRANDE DO NORTE" and "RIO GRANDE DO SUL".

```{r}
# remove accents
levels(map_states$nome) <- iconv(levels(map_states$nome),from="UTF-8",to="ASCII//TRANSLIT")
levels(BR_fire_data$state) <- iconv(levels(BR_fire_data$state),from="UTF-8",to="ASCII//TRANSLIT")

# return vector of state names in brazilmaps that are not in Kaggle
cat("In brazilmaps but not Kaggle:\n")
cat(
  paste0(
    setdiff(
      levels(map_states$nome),
      levels(BR_fire_data$state)),
    collapse=", "
    )
  )

# Equivalent, except from Kaggle to brazilmaps
cat("\n\nIn Kaggle but not brazilmaps:\n")
cat(
  paste0(
    setdiff(
      levels(BR_fire_data$state), 
      levels(map_states$nome)),
    collapse=", "
    )
  )
```

A quick check of Wikipedia tells us that that Piaui is correct, not Piau. It also appears that there is 'duplicate' data for RIO and MATO GROSSO which indicates that data for all 3 Rios and 2 Mato Grossos exist, despite their labelling. The only remaining states that aren't understood are PARA and PARANA.

Let's reload and examine how many each state occurs in the Kaggle data.

```{r}
BR_fire_data <- read.csv('../data/original.csv')
BR_fire_data %>% 
  group_by(state) %>%
  summarise(no_rows = length(state))
```

The reason for reloading was because 'Par√°' actually produced an error from our earlier transformations. PARANA must have been labelled as Paraiba, since it's the only unexplained double-count factor. 

So how should we handle this imperfect mapping? If it was complex, it might be easier to leave the data as is or to join the duplicate data. We won't do this because the problem isn't too challenging and some of the mislabelled geographies such as the three Rios are NOT close. Let's cross-reference the data with another reliable source. Note that the data does not line up exactly.

Using a visualization from [globalforestwatch](http://fires.globalforestwatch.org/report/index.html#aoitype=GLOBAL&reporttype=globalcountryreport&country=Brazil&dates=fYear-2019!fMonth-6!fDay-1!tYear-2019!tMonth-11!tDay-27), here are the total number of fires for 2003 for our states of interest:

State | GFW Fires in 2003
------------- | -------------
Rio de Janeiro | 1,786
Rio Grande do Norte | 2,139
Rio Grande do Sul | 7,037
Parana | 12,756
Paraiba | 2,190
Mato Grosso | 10,737
Mato Grosso do Sul | 150,256

Now I'll add the suspected Kaggle data to each column. Rio de Janeiro and Rio Grande do Norte have somewhat similar numbers, so the comparison will be done with 2003 and 2017 data.

State | GFW Fires,2003 | Kaggle Fires,2003 | GFW Fires,2007 | Kaggle Fires,2007
------------- | ------------- | ------------- | ------------- | -------------
Rio de Janeiro | 1,786 | 553 | 2,400 | 621 
Rio Grande do Norte | 2,139 | 1,262 | 1,230 | 463
Rio Grande do Sul | 7,037 | 3,041 | 3,488 | 983
Parana | 12,756 | 6,980 | 7,028 | 2,057
Paraiba | 2,190 | 1,381 | 1,430 | 608
Mato Grosso | 10,737 | 4,130 | 21,782 | 7,747
Mato Grosso do Sul | 150,256 | 35,406 | 164,789 | 44,406

Although there is some difference in how much the states differ, these two comparisons make it look like my inference is correct. If GRW provided me with their data, the comparison could be done with more points or with visualization to make my assumptions more convincing. 

The date column isn't all that necessary -- month and year are present, and you might notice that the data provider also didn't provide correct dates - they are all January 1 of whatever year. The column might otherwise be worth having, but that discreptency makes it useless. I'll work with the year and month which are consistant. 

After looking at the number of fires columns, you might also notice that a period is used in certain instances, often in groups and close to numbers close to 1000. This indicates they are being used in place of a comma or nothing since it's supposed to be an integer. So transformations like $1.234 \rightarrow 1234$ must be made. 

Initially, you might think replacing '.' with '' will deal with our problem, however it does not -- there are some points with only one or two decimals. They must have ended in zero. It's hard to find those with the off chance of being a multiple of 1000, but we'll cross that bridge when we get there. The change $1.23 \rightarrow 1230$ may seem daunting if:

1. You were to change all 800+ manually.
2. You are not experienced with these weird preprocessing problems.

Luckily, there's something called $\textit{Regular Expressions}$ (RegEx) that can be used to solve this. This [Medium article by Jonny Fox](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285) does a great job at introducing the topic, but I will assume the reader knows basic regex. The following are the steps used to transform the numbers in the data set:

1. Open the original csv file (data folder) in a text editor with RegEx Find & Replace.
2. Search for the desired numbers using RegEx 
   "#.#," found by "(\d)\.(\d)" where the round brackets are used for grouping
   Similarly "#.##," and "#.###," can be found using "(\\d)\.(\\d{2})" and "(\\d)\\.(\\d{2})", respectively.
3. Use your groups combined with any supplementary "0"s to properly format the numbers.
ie. "$1$200", "$1$20" and "$1$2" for the above situations.

Now how might we find a number that should actually be #.000 only the ".000" isn't there? Well, we can do a check using regex but we won't know for sure. Before doing that, sort the data by Month,Year,State so that we see the data in sequence, by state. This is important for spotting odd numbers in context. Here's the Regex search I used: 

\begin{align}
,\setminus d\{3,4\}\setminus n\setminus d*,(\setminus w*,)\{2\}\setminus d\setminus n\setminus d*,(\setminus w*,)\{2\}\setminus d\{3,4\}
\end{align}

where I'm searching for a single digit number preceeded and followed by a 3-4 digit number. The same principle can be used to search for a number like 55000.

There are 4 matches, but the only one I'd be confident changing would be the middle row below:

Year | State | Month | Number of Fires
---- | ----- | ----- | ---------------
2012 | MARANHAO | September | 7626
2012 | MARANHAO | October | 5     (should probably be 5000)
2012 | MARANHAO | November | 2010

The following also looks suspicious, but I'll attribute that to a measurement error - jumping to 5000 seems unlikely given surrounding numbers.

Year | State | Month | Number of Fires
---- | ----- | ----- | ---------------
2009 | MARANHAO | October | 1861
2009 | MARANHAO | November | 5
2009 | MARANHAO | December | 1768

I dug a bit deeper into other possible multiples of 1000 but did not see anything I previously missed.

After the transformations we layed out are complete (renaming, capitalizing, adding accents and reformatting numbers), the original data becomes amazon.csv. 
















